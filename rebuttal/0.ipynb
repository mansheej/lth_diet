{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "plt.style.use(\"default\")\n",
    "rc = {\"figure.figsize\": (4, 3), \"figure.dpi\": 150, \"figure.constrained_layout.use\": True, \"axes.grid\": True, \n",
    "      \"axes.spines.right\": False, \"axes.spines.top\": False, \"axes.linewidth\": 0.6, \"grid.linewidth\": 0.6,\n",
    "      \"xtick.major.width\": 0.6, \"ytick.major.width\": 0.6, \"xtick.major.size\": 4, \"ytick.major.size\": 4, \n",
    "      \"axes.labelsize\": 14, \"axes.titlesize\": 14, \"xtick.labelsize\": 12, \"ytick.labelsize\": 12,\n",
    "      \"axes.titlepad\": 4, \"axes.labelpad\": 2, \"xtick.major.pad\": 2, \"ytick.major.pad\": 2,\n",
    "      \"lines.linewidth\": 1.2, \"patch.linewidth\": 0}\n",
    "sns.set_theme(style='ticks', palette=sns.color_palette(\"colorblind\"), rc=rc)\n",
    "import torch\n",
    "from composer import Trainer\n",
    "from composer.algorithms import ChannelsLast\n",
    "from composer.datasets import DataLoaderHparams\n",
    "from composer.optim import SGDHparams\n",
    "from composer.optim.scheduler import ConstantScheduler\n",
    "from composer.loggers import InMemoryLogger, TQDMLogger\n",
    "from composer.utils import reproducibility\n",
    "from composer.utils.object_store import ObjectStoreProviderHparams\n",
    "from lth_diet.data import CIFAR100DataHparams\n",
    "from lth_diet.data.data_diet import SubsetByScore\n",
    "from lth_diet.models import ResNetClassifierHparams\n",
    "from tqdm.notebook import tqdm\n",
    "object_store = ObjectStoreProviderHparams('google_storage', 'prunes', 'GCS_KEY').initialize_object()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train ResNet-50 on the entire CIFAR-100 dataset for 1600 steps, saving and evaluating the model on train and test sets every 50 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP_NAME = \"resnet_50__cifar100_all_data\"\n",
    "# DEVICE = \"gpu\"\n",
    "# REPLICATE = 0\n",
    "# BATCH_SIZE = 128\n",
    "# VAL_BATCH_SIZE = 1000\n",
    "# MAX_DURATION = \"1600ba\"\n",
    "# INTERVAL = 50\n",
    "\n",
    "# MODEL_HPARAMS = ResNetClassifierHparams(num_classes=100, num_layers=50, low_res=True)\n",
    "# DATA_HPARMS = CIFAR100DataHparams(train=True)\n",
    "# VAL_DATA_HPARAMS = CIFAR100DataHparams(train=False)\n",
    "\n",
    "# os.makedirs(f\"../exps/rank_0/{EXP_NAME}/rep_{REPLICATE}\")\n",
    "# seed = 2022 * (REPLICATE + 1)\n",
    "# reproducibility.seed_all(seed)\n",
    "# model = MODEL_HPARAMS.initialize_object()\n",
    "# torch.save(model.state_dict(), f\"../exps/rank_0/{EXP_NAME}/rep_{REPLICATE}/ba_0.pt\")\n",
    "# reproducibility.seed_all(42)\n",
    "# dataloader = DATA_HPARMS.initialize_object(BATCH_SIZE, DataLoaderHparams(persistent_workers=False))\n",
    "# val_dataloader = VAL_DATA_HPARAMS.initialize_object(VAL_BATCH_SIZE, DataLoaderHparams(persistent_workers=False))\n",
    "# optimizer = SGDHparams(lr=0.1, momentum=0.9, weight_decay=0.0005, nesterov=True).initialize_object(model.parameters())\n",
    "# scheduler = [ConstantScheduler()]\n",
    "# algorithms = [ChannelsLast()]\n",
    "# loggers = [InMemoryLogger(), TQDMLogger()]\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model = model,\n",
    "#     train_dataloader=dataloader,\n",
    "#     max_duration=MAX_DURATION,\n",
    "#     eval_dataloader=val_dataloader,\n",
    "#     algorithms=algorithms,\n",
    "#     optimizers=optimizer,\n",
    "#     schedulers=scheduler,\n",
    "#     device=DEVICE,\n",
    "#     validate_every_n_batches=INTERVAL,\n",
    "#     precision=\"amp\",\n",
    "#     step_schedulers_every_batch=True,\n",
    "#     seed=seed,\n",
    "#     loggers=loggers,\n",
    "#     save_folder=f\"{EXP_NAME}/rep_{REPLICATE}\",\n",
    "#     save_name_format=\"ba_{batch}.pt\",\n",
    "#     save_interval=f\"{INTERVAL}ba\",\n",
    "# )\n",
    "\n",
    "# trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train ResNet-50 on 2048 easiest CIFAR-100 examples for 1600 steps, saving and evaluating the model on train and test sets every 50 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP_NAME = \"resnet_50__cifar100_easy_data\"\n",
    "# DEVICE = \"gpu\"\n",
    "# REPLICATE = 0\n",
    "# SUBSET_SIZE = 2048\n",
    "# BATCH_SIZE = 128\n",
    "# VAL_BATCH_SIZE = 1000\n",
    "# MAX_DURATION = \"1600ba\"\n",
    "# INTERVAL = 50\n",
    "\n",
    "# MODEL_HPARAMS = ResNetClassifierHparams(num_classes=100, num_layers=50, low_res=True)\n",
    "# DATA_TRANSFORMS = [\n",
    "#     SubsetByScore(score=\"error_norm_cifar100_resnet50_7800ba_16reps_seed789\", size=SUBSET_SIZE, class_balanced=True)\n",
    "# ]\n",
    "# DATA_HPARMS = CIFAR100DataHparams(train=True, dataset_transforms=DATA_TRANSFORMS)\n",
    "# VAL_DATA_HPARAMS = CIFAR100DataHparams(train=False)\n",
    "\n",
    "# os.makedirs(f\"../exps/rank_0/{EXP_NAME}/rep_{REPLICATE}\", exist_ok=True)\n",
    "# seed = 2022 * (REPLICATE + 1)\n",
    "# reproducibility.seed_all(seed)\n",
    "# model = MODEL_HPARAMS.initialize_object()\n",
    "# torch.save(model.state_dict(), f\"../exps/rank_0/{EXP_NAME}/rep_{REPLICATE}/ba_0.pt\")\n",
    "# reproducibility.seed_all(42)\n",
    "# dataloader = DATA_HPARMS.initialize_object(BATCH_SIZE, DataLoaderHparams(persistent_workers=False), object_store=object_store)\n",
    "# val_dataloader = VAL_DATA_HPARAMS.initialize_object(VAL_BATCH_SIZE, DataLoaderHparams(persistent_workers=False))\n",
    "# optimizer = SGDHparams(lr=0.1, momentum=0.9, weight_decay=0.0005, nesterov=True).initialize_object(model.parameters())\n",
    "# scheduler = [ConstantScheduler()]\n",
    "# algorithms = [ChannelsLast()]\n",
    "# loggers = [InMemoryLogger(), TQDMLogger()]\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model = model,\n",
    "#     train_dataloader=dataloader,\n",
    "#     max_duration=MAX_DURATION,\n",
    "#     eval_dataloader=val_dataloader,\n",
    "#     algorithms=algorithms,\n",
    "#     optimizers=optimizer,\n",
    "#     schedulers=scheduler,\n",
    "#     device=DEVICE,\n",
    "#     validate_every_n_batches=INTERVAL,\n",
    "#     validate_every_n_epochs=-1,\n",
    "#     precision=\"amp\",\n",
    "#     step_schedulers_every_batch=True,\n",
    "#     seed=seed,\n",
    "#     loggers=loggers,\n",
    "#     save_folder=f\"{EXP_NAME}/rep_{REPLICATE}\",\n",
    "#     save_name_format=\"ba_{batch}.pt\",\n",
    "#     save_interval=f\"{INTERVAL}ba\",\n",
    "# )\n",
    "\n",
    "# trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(step):\n",
    "    \n",
    "    model = MODEL_HPARAMS.initialize_object()\n",
    "    state_dict = torch.load(f\"../exps/rank_0/{EXP_NAME}/rep_{REPLICATE}/ba_{step}.pt\")[\"state\"][\"model\"]\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.cuda()\n",
    "    model = model.eval()\n",
    "    \n",
    "    dataloader = DATA_HPARAMS.initialize_object(\n",
    "        BATCH_SIZE, DataLoaderHparams(persistent_workers=False), object_store=object_store\n",
    "    )\n",
    "    \n",
    "    loss, acc = 0, 0\n",
    "    for x, y in tqdm(dataloader):\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        logits = model((x, y))\n",
    "        loss += model.loss(logits, (x, y))\n",
    "        acc += (y == logits.argmax(-1)).sum()\n",
    "        \n",
    "    loss = loss.item() / len(dataloader.dataset)\n",
    "    acc = acc.item() / len(dataloader.dataset)\n",
    "    \n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = {}\n",
    "RESULTS[\"steps\"] = np.arange(50, 1650, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test loss and accuracy of model trained on all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP_NAME = \"resnet_50__cifar100_all_data\"\n",
    "# DEVICE = \"gpu\"\n",
    "# REPLICATE = 0\n",
    "# BATCH_SIZE = 1000\n",
    "\n",
    "# MODEL_HPARAMS = ResNetClassifierHparams(num_classes=100, num_layers=50, low_res=True)\n",
    "# DATA_HPARAMS = CIFAR100DataHparams(train=False)\n",
    "\n",
    "# losses, accs = [], []\n",
    "# for step in RESULTS[\"steps\"]:\n",
    "#     print(step)\n",
    "#     loss, acc = evaluate(step)\n",
    "#     losses.append(loss)\n",
    "#     accs.append(acc)\n",
    "    \n",
    "# losses, accs = np.array(losses), np.array(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS[\"train_all_test_loss\"] = losses\n",
    "# RESULTS[\"train_all_test_acc\"] = accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test loss and accuracy of model trained on easy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP_NAME = \"resnet_50__cifar100_easy_data\"\n",
    "# DEVICE = \"gpu\"\n",
    "# REPLICATE = 0\n",
    "# BATCH_SIZE = 1000\n",
    "\n",
    "# MODEL_HPARAMS = ResNetClassifierHparams(num_classes=100, num_layers=50, low_res=True)\n",
    "# DATA_HPARAMS = CIFAR100DataHparams(train=False)\n",
    "\n",
    "# losses, accs = [], []\n",
    "# for step in RESULTS[\"steps\"]:\n",
    "#     print(step)\n",
    "#     loss, acc = evaluate(step)\n",
    "#     losses.append(loss)\n",
    "#     accs.append(acc)\n",
    "    \n",
    "# losses, accs = np.array(losses), np.array(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS[\"train_easy_test_loss\"] = losses\n",
    "# RESULTS[\"train_easy_test_acc\"] = accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train loss and accuracy on all data of model trained on all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP_NAME = \"resnet_50__cifar100_all_data\"\n",
    "# DEVICE = \"gpu\"\n",
    "# REPLICATE = 0\n",
    "# BATCH_SIZE = 1000\n",
    "\n",
    "# MODEL_HPARAMS = ResNetClassifierHparams(num_classes=100, num_layers=50, low_res=True)\n",
    "# DATA_HPARAMS = CIFAR100DataHparams(train=True, shuffle=False, drop_last=False, no_augment=True)\n",
    "\n",
    "# losses, accs = [], []\n",
    "# for step in RESULTS[\"steps\"]:\n",
    "#     print(step)\n",
    "#     loss, acc = evaluate(step)\n",
    "#     losses.append(loss)\n",
    "#     accs.append(acc)\n",
    "    \n",
    "# losses, accs = np.array(losses), np.array(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS[\"train_all_train_all_loss\"] = losses\n",
    "# RESULTS[\"train_all_train_all_acc\"] = accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train loss and accuracy on all data of model trained on easy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP_NAME = \"resnet_50__cifar100_easy_data\"\n",
    "# DEVICE = \"gpu\"\n",
    "# REPLICATE = 0\n",
    "# BATCH_SIZE = 1000\n",
    "\n",
    "# MODEL_HPARAMS = ResNetClassifierHparams(num_classes=100, num_layers=50, low_res=True)\n",
    "# DATA_HPARAMS = CIFAR100DataHparams(train=True, shuffle=False, drop_last=False, no_augment=True)\n",
    "\n",
    "# losses, accs = [], []\n",
    "# for step in RESULTS[\"steps\"]:\n",
    "#     print(step)\n",
    "#     loss, acc = evaluate(step)\n",
    "#     losses.append(loss)\n",
    "#     accs.append(acc)\n",
    "    \n",
    "# losses, accs = np.array(losses), np.array(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS[\"train_easy_train_all_loss\"] = losses\n",
    "# RESULTS[\"train_easy_train_all_acc\"] = accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train loss and accuracy on easy data of model trained on all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP_NAME = \"resnet_50__cifar100_all_data\"\n",
    "# DEVICE = \"gpu\"\n",
    "# REPLICATE = 0\n",
    "# SUBSET_SIZE = 2048\n",
    "# BATCH_SIZE = 1000\n",
    "\n",
    "# MODEL_HPARAMS = ResNetClassifierHparams(num_classes=100, num_layers=50, low_res=True)\n",
    "# DATA_TRANSFORMS = [\n",
    "#     SubsetByScore(score=\"error_norm_cifar100_resnet50_7800ba_16reps_seed789\", size=SUBSET_SIZE, class_balanced=True)\n",
    "# ]\n",
    "# DATA_HPARAMS = CIFAR100DataHparams(\n",
    "#     train=True, shuffle=False, drop_last=False, no_augment=True, dataset_transforms=DATA_TRANSFORMS\n",
    "# )\n",
    "\n",
    "# losses, accs = [], []\n",
    "# for step in RESULTS[\"steps\"]:\n",
    "#     print(step)\n",
    "#     loss, acc = evaluate(step)\n",
    "#     losses.append(loss)\n",
    "#     accs.append(acc)\n",
    "    \n",
    "# losses, accs = np.array(losses), np.array(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS[\"train_all_train_easy_loss\"] = losses\n",
    "# RESULTS[\"train_all_train_easy_acc\"] = accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train loss and accuracy on easy data of model trained on easy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP_NAME = \"resnet_50__cifar100_easy_data\"\n",
    "# DEVICE = \"gpu\"\n",
    "# REPLICATE = 0\n",
    "# SUBSET_SIZE = 2048\n",
    "# BATCH_SIZE = 1000\n",
    "\n",
    "# MODEL_HPARAMS = ResNetClassifierHparams(num_classes=100, num_layers=50, low_res=True)\n",
    "# DATA_TRANSFORMS = [\n",
    "#     SubsetByScore(score=\"error_norm_cifar100_resnet50_7800ba_16reps_seed789\", size=SUBSET_SIZE, class_balanced=True)\n",
    "# ]\n",
    "# DATA_HPARAMS = CIFAR100DataHparams(\n",
    "#     train=True, shuffle=False, drop_last=False, no_augment=True, dataset_transforms=DATA_TRANSFORMS\n",
    "# )\n",
    "\n",
    "# losses, accs = [], []\n",
    "# for step in RESULTS[\"steps\"]:\n",
    "#     print(step)\n",
    "#     loss, acc = evaluate(step)\n",
    "#     losses.append(loss)\n",
    "#     accs.append(acc)\n",
    "    \n",
    "# losses, accs = np.array(losses), np.array(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS[\"train_easy_train_easy_loss\"] = losses\n",
    "# RESULTS[\"train_easy_train_easy_acc\"] = accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(RESULTS, \"results_0.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(RESULTS[\"steps\"], RESULTS[\"train_all_test_acc\"], '.-', label=\"Trained on All Data\")\n",
    "# plt.plot(RESULTS[\"steps\"], RESULTS[\"train_easy_test_acc\"], '.-', label=\"Trained on Easy Data\")\n",
    "# # plt.legend()\n",
    "# plt.xticks([0, 400, 800, 1200, 1600])\n",
    "# plt.xlabel(\"Step\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"Performance on Test Data\")\n",
    "# plt.savefig(\"test.svg\")\n",
    "# plt.show()\n",
    "# plt.figure()\n",
    "# plt.plot(RESULTS[\"steps\"], RESULTS[\"train_all_train_all_acc\"], '.-', label=\"Trained on All Data\")\n",
    "# plt.plot(RESULTS[\"steps\"], RESULTS[\"train_easy_train_all_acc\"], '.-', label=\"Trained on Easy Data\")\n",
    "# # plt.legend()\n",
    "# plt.xticks([0, 400, 800, 1200, 1600])\n",
    "# plt.xlabel(\"Step\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"Performance on All Train Data\")\n",
    "# plt.savefig(\"train_all.svg\")\n",
    "# plt.show()\n",
    "# plt.figure()\n",
    "# plt.plot(RESULTS[\"steps\"], RESULTS[\"train_all_train_easy_acc\"], '.-', label=\"Trained on All Data\")\n",
    "# plt.plot(RESULTS[\"steps\"], RESULTS[\"train_easy_train_easy_acc\"], '.-', label=\"Trained on Easy Data\")\n",
    "# # plt.legend()\n",
    "# plt.xticks([0, 400, 800, 1200, 1600])\n",
    "# plt.xlabel(\"Step\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"Performance on Easy Train Data\")\n",
    "# plt.savefig(\"train_easy.svg\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_sims(step):\n",
    "    seed = 2022 * (REPLICATE + 1)\n",
    "    reproducibility.seed_all(seed)\n",
    "    model = MODEL_HPARAMS.initialize_object()\n",
    "    model.load_state_dict(torch.load(f\"../exps/rank_0/{EXP_NAME}/rep_{REPLICATE}/ba_{step}.pt\")[\"state\"][\"model\"])\n",
    "    model = model.cuda()\n",
    "    model = model.eval()\n",
    "\n",
    "    dataloader = DATA_HPARMS.initialize_object(\n",
    "        BATCH_SIZE, DataLoaderHparams(persistent_workers=False), object_store=object_store\n",
    "    )\n",
    "\n",
    "    grads = []\n",
    "    for x, y in tqdm(dataloader):\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        model.zero_grad()\n",
    "        loss = model.loss(model((x, y)), (x, y))\n",
    "        loss.backward()\n",
    "        grads.append(torch.cat([p.grad.detach().flatten() for p in model.parameters()]).cpu())\n",
    "        if len(grads) == N_GRADS:\n",
    "            break\n",
    "    grads = torch.stack(grads)\n",
    "    mean_grad = grads.mean(0)\n",
    "    return torch.nn.functional.cosine_similarity(grads, mean_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_sims = {}\n",
    "grad_sims[\"steps\"] = np.arange(50, 450, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"resnet_50__cifar100_all_data\"\n",
    "REPLICATE = 0\n",
    "BATCH_SIZE = 128\n",
    "N_GRADS = 16\n",
    "\n",
    "MODEL_HPARAMS = ResNetClassifierHparams(num_classes=100, num_layers=50, low_res=True)\n",
    "DATA_HPARMS = CIFAR100DataHparams(train=True, shuffle=True, drop_last=False, no_augment=True)\n",
    "\n",
    "grad_sims[\"all\"] = [get_grad_sims(step) for step in grad_sims[\"steps\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"resnet_50__cifar100_easy_data\"\n",
    "REPLICATE = 0\n",
    "SUBSET_SIZE = 2048\n",
    "BATCH_SIZE = 128\n",
    "N_GRADS = 16\n",
    "\n",
    "DATA_TRANSFORMS = [\n",
    "    SubsetByScore(score=\"error_norm_cifar100_resnet50_7800ba_16reps_seed789\", size=SUBSET_SIZE, class_balanced=True)\n",
    "]\n",
    "MODEL_HPARAMS = ResNetClassifierHparams(num_classes=100, num_layers=50, low_res=True)\n",
    "DATA_HPARMS = CIFAR100DataHparams(\n",
    "    train=True, shuffle=True, drop_last=False, no_augment=True, dataset_transforms=DATA_TRANSFORMS\n",
    ")\n",
    "\n",
    "grad_sims[\"easy\"] = [get_grad_sims(step) for step in grad_sims[\"steps\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_sims[\"all\"] = np.stack([np.array(gs) for gs in grad_sims[\"all\"]])\n",
    "grad_sims[\"easy\"] = np.stack([np.array(gs) for gs in grad_sims[\"easy\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(grad_sims[\"steps\"], grad_sims[\"all\"].mean(1), yerr=grad_sims[\"all\"].std(1), marker=\"o\", capsize=4, alpha=0.9)\n",
    "plt.errorbar(grad_sims[\"steps\"], grad_sims[\"easy\"].mean(1), yerr=grad_sims[\"easy\"].std(1), marker=\"o\", capsize=4, alpha=0.9)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Cosine Similarity\")\n",
    "plt.ylim(-0.4, 1)\n",
    "plt.xlim(0, 420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('lth_diet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6ede5693076468011a9b06db16dff54c2e2dab3909e284e06b92eee2c289d4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
