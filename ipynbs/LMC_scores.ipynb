{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.algorithms import ChannelsLastHparams\n",
    "from composer.callbacks import LRMonitorHparams\n",
    "from composer.core.time import Time\n",
    "from composer.core.types import DataLoader\n",
    "from composer.datasets import DataLoaderHparams\n",
    "from composer.loggers import WandBLoggerHparams\n",
    "from composer.models import ComposerClassifier\n",
    "from composer.optim import (SGDHparams, ConstantSchedulerHparams, CosineAnnealingSchedulerHparams, \n",
    "                            CosineAnnealingWithWarmupSchedulerHparams, MultiStepSchedulerHparams, \n",
    "                            MultiStepWithWarmupSchedulerHparams)\n",
    "from composer.trainer import Trainer, TrainerHparams\n",
    "from composer.utils.object_store import ObjectStoreProviderHparams, ObjectStoreProvider\n",
    "from copy import deepcopy\n",
    "from lth_diet.data import CIFAR10DataHparams, DataHparams, CINIC10DataHparams\n",
    "from lth_diet.exps import LotteryExperiment, LotteryRetrainExperiment\n",
    "from lth_diet.models import ResNetCIFARClassifierHparams, ClassifierHparams\n",
    "from lth_diet.pruning import Mask, PrunedClassifier, PruningHparams\n",
    "from lth_diet.pruning.pruned_classifier import prunable_layer_names\n",
    "from lth_diet.utils import utils\n",
    "from numpy.typing import NDArray\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Callable, Dict, Tuple\n",
    "plt.style.use(\"default\")\n",
    "rc = {\"figure.figsize\": (4, 3), \"figure.dpi\": 150, \"figure.constrained_layout.use\": True, \"axes.grid\": True, \n",
    "      \"axes.spines.right\": False, \"axes.spines.top\": False, \"axes.linewidth\": 0.6, \"grid.linewidth\": 0.6,\n",
    "      \"xtick.major.width\": 0.6, \"ytick.major.width\": 0.6, \"xtick.major.size\": 4, \"ytick.major.size\": 4, \n",
    "      \"axes.labelsize\": 11, \"axes.titlesize\": 11, \"xtick.labelsize\": 10, \"ytick.labelsize\": 10,\n",
    "      \"axes.titlepad\": 4, \"axes.labelpad\": 2, \"xtick.major.pad\": 2, \"ytick.major.pad\": 2,\n",
    "      \"lines.linewidth\": 1.2, \"patch.linewidth\": 0}\n",
    "sns.set_theme(style='ticks', palette=sns.color_palette(\"colorblind\"), rc=rc)\n",
    "object_store = ObjectStoreProviderHparams('google_storage', 'prunes', 'GCS_KEY').initialize_object()\n",
    "bucket_dir = os.environ['OBJECT_STORE_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = f\"../configs/lottery_cinic10_retrain.yaml\"\n",
    "exp = LotteryRetrainExperiment.create(f=config, cli_args=False)\n",
    "load_replicates = [0, 1, 2, 3]\n",
    "replicates = [0, 1]\n",
    "rewinding_steps = [400, 800, 1600]\n",
    "model_hparams = ResNetCIFARClassifierHparams(10, 56)\n",
    "train_data = CINIC10DataHparams(True,False,False,True,).initialize_object(1000, DataLoaderHparams(persistent_workers=False))\n",
    "test_data = CINIC10DataHparams(False).initialize_object(1000, DataLoaderHparams(persistent_workers=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def losses_and_state_dict(location, name, train_data):\n",
    "    state_dict = utils.load_object(location, name, object_store, torch.load)\n",
    "    model = model_hparams.initialize_object()\n",
    "    model.module.load_state_dict(state_dict)\n",
    "    print(f\"      Loaded {utils.get_object_name(location, name)}\")\n",
    "    losses = []\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    for batch in train_data:\n",
    "        batch = batch[0].cuda(), batch[1].cuda()\n",
    "        logits = model(batch)\n",
    "        losses.append(torch.nn.CrossEntropyLoss(reduction=\"none\")(logits, batch[1])) \n",
    "    losses = torch.cat(losses).cpu().numpy()\n",
    "    print(f\"      Evaluated {utils.get_object_name(location, name)}\")\n",
    "    return losses, state_dict\n",
    "\n",
    "def losses_only(state_dict, train_data):\n",
    "    model = model_hparams.initialize_object()\n",
    "    model.module.load_state_dict(state_dict)\n",
    "    losses = []\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    for batch in train_data:\n",
    "        batch = batch[0].cuda(), batch[1].cuda()\n",
    "        logits = model(batch)\n",
    "        losses.append(torch.nn.CrossEntropyLoss(reduction=\"none\")(logits, batch[1])) \n",
    "    losses = torch.cat(losses).cpu().numpy()\n",
    "    print(f\"      Evaluated midpoint\")\n",
    "    return losses\n",
    "\n",
    "def midpoint(state_dict: Dict, state_dict_: Dict) -> Dict:\n",
    "    \"\"\"Return the midpoint between two state dicts\"\"\"\n",
    "    state_dict__ = {}\n",
    "    for k, v in state_dict.items():\n",
    "        state_dict__[k] = (v + state_dict_[k]) / 2\n",
    "    return state_dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_barriers = np.zeros((len(rewinding_steps), len(load_replicates), 3, 180000))\n",
    "for i, rstep in enumerate(rewinding_steps):\n",
    "    print(\"rewinding step:\", rstep)\n",
    "    for j, lrep in enumerate(load_replicates):\n",
    "        print(\"  replicate:\", lrep)\n",
    "        # setup experiment\n",
    "        exp.load_exp.rewinding_steps = f\"{rstep}ba\"\n",
    "        exp.load_replicate = lrep\n",
    "        # parent\n",
    "        location = f\"{utils.get_hash(exp.load_exp.name)}/replicate_{lrep}/level_0/main\"\n",
    "        losses_, state_dict_ = losses_and_state_dict(location, \"model_final.pt\", train_data)\n",
    "        # child 0 \n",
    "        location = f\"{utils.get_hash(exp.load_exp.name)}/replicate_{lrep}/level_0/{utils.get_hash(exp.name)}/replicate_{0}/main\"\n",
    "        losses_0, state_dict_0 = losses_and_state_dict(location, \"model_final.pt\", train_data)\n",
    "        # child 0 \n",
    "        location = f\"{utils.get_hash(exp.load_exp.name)}/replicate_{lrep}/level_0/{utils.get_hash(exp.name)}/replicate_{1}/main\"\n",
    "        losses_1, state_dict_1 = losses_and_state_dict(location, \"model_final.pt\", train_data)\n",
    "        # midpoint 0\n",
    "        state_dict = midpoint(state_dict_, state_dict_0)\n",
    "        losses = losses_only(state_dict, train_data)\n",
    "        train_loss_barriers[i, j, 0] = losses - (losses_ + losses_0) / 2\n",
    "        # midpoint 1\n",
    "        state_dict = midpoint(state_dict_, state_dict_1)\n",
    "        losses = losses_only(state_dict, train_data)\n",
    "        train_loss_barriers[i, j, 1] = losses - (losses_ + losses_1) / 2\n",
    "        # midpoint 2\n",
    "        state_dict = midpoint(state_dict_0, state_dict_1)\n",
    "        losses = losses_only(state_dict, train_data)\n",
    "        train_loss_barriers[i, j, 2] = losses - (losses_0 + losses_1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_barriers.mean(1).mean(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_norm = utils.load_object(\"scores\", \"error_norm_cinic10_resnet56_8000ba_10reps_seed1234.npy\", object_store, np.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(error_norm, train_loss_barriers.mean(1).mean(1)[0], '.', alpha=0.01)\n",
    "plt.ylim(-4, 8)\n",
    "plt.title(\"Rewind: 400ba\")\n",
    "plt.xlabel(\"Error Norm\")\n",
    "plt.ylabel(\"Train Loss Barrier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(error_norm, train_loss_barriers.mean(1).mean(1)[1], '.', alpha=0.01)\n",
    "plt.ylim(-4, 8)\n",
    "plt.title(\"Rewind: 800ba\")\n",
    "plt.xlabel(\"Error Norm\")\n",
    "plt.ylabel(\"Train Loss Barrier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(error_norm, train_loss_barriers.mean(1).mean(1)[2], '.', alpha=0.01)\n",
    "plt.ylim(-4, 8)\n",
    "plt.title(\"Rewind: 1600ba\")\n",
    "plt.xlabel(\"Error Norm\")\n",
    "plt.ylabel(\"Train Loss Barrier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_object(train_loss_barriers.mean(1).mean(1)[2], \"scores\", \"cinic10_lmc_1600ba.npy\", object_store, lambda x, y: np.save(y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmc = utils.load_object(\"scores\", \"cinic10_lmc_400ba.npy\", object_store, np.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(error_norm, lmc, '.', alpha=0.01)\n",
    "plt.ylim(-4, 8)\n",
    "plt.title(\"Rewind: 1600ba\")\n",
    "plt.xlabel(\"Error Norm\")\n",
    "plt.ylabel(\"Train Loss Barrier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6ede5693076468011a9b06db16dff54c2e2dab3909e284e06b92eee2c289d4c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('lth_diet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
